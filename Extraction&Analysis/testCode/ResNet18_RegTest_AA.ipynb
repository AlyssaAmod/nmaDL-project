{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"ResNet18_RegTest_AA.ipynb","provenance":[{"file_id":"https://github.com/eldhusfifl07/nmaDL-project/blob/main/ResNet18_Test_AA.ipynb","timestamp":1628868632890}],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"zk6ArWQiYJOA"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-K6jvotMP0L0","cellView":"form","executionInfo":{"status":"ok","timestamp":1629215288882,"user_tz":-120,"elapsed":9120,"user":{"displayName":"Alyssa Amod","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GicxUG-gq97W97kRINYSq3tUImLdxKV4ftHgTxPTw=s64","userId":"15702157771478370556"}}},"source":["#@title Installations\n","%%capture\n","\n","!pip install nilearn\n","!pip install decord\n","\n","!git clone https://github.com/HuthLab/speechmodeltutorial.git\n","# %cd /content/nmaDL-project/resnetBasline"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2eb_A2AozKNm","executionInfo":{"status":"ok","timestamp":1629215485617,"user_tz":-120,"elapsed":627,"user":{"displayName":"Alyssa Amod","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GicxUG-gq97W97kRINYSq3tUImLdxKV4ftHgTxPTw=s64","userId":"15702157771478370556"}},"outputId":"6ddc6e6a-ceff-4a77-e184-9c142eb79dc2"},"source":["%cd '/content/speechmodeltutorial'\n","\n","from ridge import bootstrap_ridge\n","\n","%cd '/content/'"],"execution_count":4,"outputs":[{"output_type":"stream","text":["/content/speechmodeltutorial\n","/content\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"bg1MwR9HzlQl"},"source":["bootstrap_ridge"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Lpai-RAGYr81"},"source":["current directory set to\n","\n","```\n","# !git clone https://github.com/eldhusfifl07/nmaDL-project.git\n","```\n","Change directory to Baseline Run for output\n"]},{"cell_type":"code","metadata":{"id":"_fBMjxonWoYY","cellView":"form","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1629125777857,"user_tz":-270,"elapsed":7099,"user":{"displayName":"yamin bagheri","photoUrl":"","userId":"07512743611623987262"}},"outputId":"535a5959-bd55-4f76-c712-589ab5fe5186"},"source":["#@title Imports\n","import glob\n","import numpy as np\n","import pandas as pd\n","import urllib\n","import torch\n","import cv2\n","import os\n","import argparse\n","import time\n","import random\n","import matplotlib.pyplot as plt\n","import nibabel as nib\n","import pickle\n","import sys \n","sys.path.append(\"../\")\n","from tqdm import tqdm\n","# from utils import *\n","\n","from torchvision import transforms as trn\n","import torchvision.models as models\n","import torch.nn as nn\n","import torch.utils.model_zoo as model_zoo\n","import torch.nn.functional as F\n","from torch.autograd import Variable as V\n","from torch.utils.data import Dataset, DataLoader\n","\n","import albumentations as A\n","# from albumentations.pytorch import ToTensorV2\n","\n","from PIL import Image\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.decomposition import PCA, IncrementalPCA\n","from scipy.stats import ortho_group\n","\n","from nilearn import datasets\n","from nilearn import surface\n","from nilearn import plotting\n","\n","from decord import VideoReader\n","from decord import cpu\n","\n","\n","\n","\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/nilearn/datasets/__init__.py:89: FutureWarning: Fetchers from the nilearn.datasets module will be updated in version 0.9 to return python strings instead of bytes and Pandas dataframes instead of Numpy arrays.\n","  \"Numpy arrays.\", FutureWarning)\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"ebHgr_uMRPU1"},"source":["Get the dropbox link to download dataset by filling this [google form](https://forms.gle/qq9uqqu6SwN8ytxQ9). "]},{"cell_type":"code","metadata":{"id":"PL_VxeyuoeKu"},"source":["#@title Enter the download link and run the cell\n","download_link = 'https://www.dropbox.com/s/agxyxntrbwko7t1/participants_data.zip?dl=1' #@param {type:\"string\"}\n","os.environ[\"download_link\"] = download_link\n","!echo $download_link \n","!wget -O participants_data.zip -c $download_link  \n","!unzip participants_data.zip\n","!wget -O example.nii -c https://github.com/Neural-Dynamics-of-Visual-Cognition-FUB/Algonauts2021_devkit/raw/main/example.nii\n","!wget -c https://raw.githubusercontent.com/Neural-Dynamics-of-Visual-Cognition-FUB/Algonauts2021_devkit/main/class_names_ImageNet.txt"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bWnlGkTwR0kL"},"source":["#Loading fMRI data\n","The trainig dataset contains 1,000 3-second videos + fMRI human brain data of 10 subjects in response to viewing videos from this set. \n","\n","The ROI data is provided for 9 ROIs of the visual brain (V1, V2, V3, V4, LOC, EBA, FFA, STS, PPA) in a Pickle file (e.g. V1.pkl) that contains a num_videos x num_repetitions x num_voxels matrix. For each ROI, we selected voxels that showed significant split-half reliability.\n","\n","The whole brain data is provided for selected voxels across the whole brain showing reliable responses to videos in a Pickle file (e.g. WB.pkl) that contains a num_videos x num_repetitions x num_voxels matrix.\n","\n","In this section, we demonstrate how to load fMRI data for a given ROI. "]},{"cell_type":"code","metadata":{"id":"Q5rzU3l5Sjmq","cellView":"form"},"source":["#@title Utility functions for data loading - need to download data for these functions\n","def save_dict(di_, filename_):\n","    with open(filename_, 'wb') as f:\n","        pickle.dump(di_, f)\n","\n","def load_dict(filename_):\n","    with open(filename_, 'rb') as f:\n","        u = pickle._Unpickler(f)\n","        u.encoding = 'latin1'\n","        ret_di = u.load()\n","    return ret_di\n","\n","def visualize_activity(vid_id,sub):\n","  fmri_dir = './participants_data_v2021' \n","  track = \"full_track\"\n","  results_dir = '/content/drive/MyDrive/Colab Notebooks/DumboOctopi/AlgonautsDataExtraction/resnetBasline'\n","  track_dir = os.path.join(fmri_dir, track) \n","  sub_fmri_dir = os.path.join(track_dir, sub)\n","  fmri_train_all,voxel_mask = get_fmri(sub_fmri_dir,\"WB\") \n","  visual_mask_3D = np.zeros((78,93,71))\n","  visual_mask_3D[voxel_mask==1]= fmri_train_all[vid_id,:]\n","  brain_mask = '/content/example.nii'\n","  nii_save_path =  os.path.join(results_dir, 'vid_activity.nii')\n","  saveasnii(brain_mask,nii_save_path,visual_mask_3D)\n","  plotting.plot_glass_brain(nii_save_path,\n","                          title='fMRI response',plot_abs=False,\n","                          display_mode='lyr',colorbar=True)\n","\n","def get_activations(activations_dir, layer_name):\n","    \"\"\"This function loads neural network features/activations (preprocessed using PCA) into a\n","    numpy array according to a given layer.\n","    Parameters\n","    ----------\n","    activations_dir : str\n","        Path to PCA processed Neural Network features\n","    layer_name : str\n","        which layer of the neural network to load,\n","    Returns\n","    -------\n","    train_activations : np.array\n","        matrix of dimensions #train_vids x #pca_components\n","        containing activations of train videos\n","    test_activations : np.array\n","        matrix of dimensions #test_vids x #pca_components\n","        containing activations of test videos\n","    \"\"\"\n","\n","    train_file = os.path.join(activations_dir,\"train_\" + layer_name + \".npy\")\n","    test_file = os.path.join(activations_dir,\"test_\" + layer_name + \".npy\")\n","    train_activations = np.load(train_file)\n","    test_activations = np.load(test_file)\n","    scaler = StandardScaler()\n","    train_activations = scaler.fit_transform(train_activations)\n","    test_activations = scaler.fit_transform(test_activations)\n","\n","    return train_activations, test_activations\n","\n","def get_fmri(fmri_dir, ROI):\n","    \"\"\"This function loads fMRI data into a numpy array for to a given ROI.\n","    Parameters\n","    ----------\n","    fmri_dir : str\n","        path to fMRI data.\n","    ROI : str\n","        name of ROI.\n","    Returns\n","    -------\n","    np.array\n","        matrix of dimensions #train_vids x #repetitions x #voxels\n","        containing fMRI responses to train videos of a given ROI\n","    \"\"\"\n","\n","\n","    # Loading ROI data\n","    ROI_file = os.path.join(fmri_dir, ROI + \".pkl\")\n","    ROI_data = load_dict(ROI_file)\n","\n","    # averaging ROI data across repetitions\n","    ROI_data_train = np.mean(ROI_data[\"train\"], axis = 1)\n","    if ROI == \"WB\":\n","        voxel_mask = ROI_data['voxel_mask']\n","        return ROI_data_train, voxel_mask\n","\n","    return ROI_data_train\n","\n","def saveasnii(brain_mask,nii_save_path,nii_data):\n","    img = nib.load(brain_mask)\n","    nii_img = nib.Nifti1Image(nii_data, img.affine, img.header)\n","    nib.save(nii_img, nii_save_path)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JGVyd8Bg370I"},"source":["# Voxel Wise Encoding\n","\n","The approach has three steps: \n","\n","1. Features of a computer vision model to videos are extracted (Fig. 1A). This changes the format of the data (from pixels to model features) and typically reduces the dimensionality of the data. The features of a given model are interpreted as a potential hypothesis about the features that a given brain area might be using to represent the stimulus. \n","2. Features of the computational model are linearly mapped onto each voxel's responses (Fig. 1B) using the training set provided. This step is necessary as there is not necessarily a one-to-one mapping between voxels and model features. Instead, each voxel's response is hypothesized to correspond to a weighted combination of activations of multiple features of the model. \n","3. The estimated mapping from the training dataset is applied on the model features corresponding to videos in the test set to predict synthetic brain data (Fig. 1C). The predicted synthetic brain data is then compared against the ground-truth left out brain data in the testing set. \n","\n"]},{"cell_type":"code","metadata":{"id":"3ZhHPftEewNf","cellView":"form"},"source":["#@title Ridge regression function with bootstraping\n","import scipy\n","from functools import reduce\n","import numpy as np\n","import logging\n","import random\n","import itertools as itools\n","\n","def mult_diag(d, mtx, left=True):\n","    \"\"\"Multiply a full matrix by a diagonal matrix.\n","    This function should always be faster than dot.\n","\n","    Input:\n","      d -- 1D (N,) array (contains the diagonal elements)\n","      mtx -- 2D (N,N) array\n","\n","    Output:\n","      mult_diag(d, mts, left=True) == dot(diag(d), mtx)\n","      mult_diag(d, mts, left=False) == dot(mtx, diag(d))\n","    \n","    By Pietro Berkes\n","    From http://mail.scipy.org/pipermail/numpy-discussion/2007-March/026807.html\n","    \"\"\"\n","    if left:\n","        return (d*mtx.T).T\n","    else:\n","        return d*mtx\n","\n","\n","def counter(iterable, countevery=100, total=None, logger=logging.getLogger(\"counter\")):\n","    \"\"\"Logs a status and timing update to [logger] every [countevery] draws from [iterable].\n","    If [total] is given, log messages will include the estimated time remaining.\n","    \"\"\"\n","    start_time = time.time()\n","\n","    ## Check if the iterable has a __len__ function, use it if no total length is supplied\n","    if total is None:\n","        if hasattr(iterable, \"__len__\"):\n","            total = len(iterable)\n","    \n","    for count, thing in enumerate(iterable):\n","        yield thing\n","        \n","        if not count%countevery:\n","            current_time = time.time()\n","            rate = float(count+1)/(current_time-start_time)\n","\n","            if rate>1: ## more than 1 item/second\n","                ratestr = \"%0.2f items/second\"%rate\n","            else: ## less than 1 item/second\n","                ratestr = \"%0.2f seconds/item\"%(rate**-1)\n","            \n","            if total is not None:\n","                remitems = total-(count+1)\n","                remtime = remitems/rate\n","                timestr = \", %s remaining\" % time.strftime('%H:%M:%S', time.gmtime(remtime))\n","                itemstr = \"%d/%d\"%(count+1, total)\n","            else:\n","                timestr = \"\"\n","                itemstr = \"%d\"%(count+1)\n","\n","            formatted_str = \"%s items complete (%s%s)\"%(itemstr,ratestr,timestr)\n","            if logger is None:\n","                print (formatted_str)\n","            else:\n","                logger.info(formatted_str)\n","\n","zs = lambda v: (v-v.mean(0))/v.std(0) ## z-score function\n","\n","\n","def ridge_corr(Rstim, Pstim, Rresp, Presp, alphas, normalpha=False, dtype=np.single, corrmin=0.2,\n","               singcutoff=1e-10, use_corr=True, logger=logging.getLogger(\"ridge_corr\")):\n","    \"\"\"Uses ridge regression to find a linear transformation of [Rstim] that approximates [Rresp].\n","    Then tests by comparing the transformation of [Pstim] to [Presp]. This procedure is repeated\n","    for each regularization parameter alpha in [alphas]. The correlation between each prediction and\n","    each response for each alpha is returned. Note that the regression weights are NOT returned.\n","    Parameters\n","    ----------\n","    Rstim : array_like, shape (TR, N)\n","        Training stimuli with TR time points and N features. Each feature should be Z-scored across time.\n","    Pstim : array_like, shape (TP, N)\n","        Test stimuli with TP time points and N features. Each feature should be Z-scored across time.\n","    Rresp : array_like, shape (TR, M)\n","        Training responses with TR time points and M responses (voxels, neurons, what-have-you).\n","        Each response should be Z-scored across time.\n","    Presp : array_like, shape (TP, M)\n","        Test responses with TP time points and M responses.\n","    alphas : list or array_like, shape (A,)\n","        Ridge parameters to be tested. Should probably be log-spaced. np.logspace(0, 3, 20) works well.\n","    normalpha : boolean\n","        Whether ridge parameters should be normalized by the Frobenius norm of Rstim. Good for\n","        comparing models with different numbers of parameters.\n","    dtype : np.dtype\n","        All data will be cast as this dtype for computation. np.single is used by default for memory\n","        efficiency.\n","    corrmin : float in [0..1]\n","        Purely for display purposes. After each alpha is tested, the number of responses with correlation\n","        greater than corrmin minus the number of responses with correlation less than negative corrmin\n","        will be printed. For long-running regressions this vague metric of non-centered skewness can\n","        give you a rough sense of how well the model is working before it's done.\n","    singcutoff : float\n","        The first step in ridge regression is computing the singular value decomposition (SVD) of the\n","        stimulus Rstim. If Rstim is not full rank, some singular values will be approximately equal\n","        to zero and the corresponding singular vectors will be noise. These singular values/vectors\n","        should be removed both for speed (the fewer multiplications the better!) and accuracy. Any\n","        singular values less than singcutoff will be removed.\n","    use_corr : boolean\n","        If True, this function will use correlation as its metric of model fit. If False, this function\n","        will instead use variance explained (R-squared) as its metric of model fit. For ridge regression\n","        this can make a big difference -- highly regularized solutions will have very small norms and\n","        will thus explain very little variance while still leading to high correlations, as correlation\n","        is scale-free while R**2 is not.\n","    Returns\n","    -------\n","    Rcorrs : array_like, shape (A, M)\n","        The correlation between each predicted response and each column of Presp for each alpha.\n","    \n","    \"\"\"\n","    ## Calculate SVD of stimulus matrix\n","    logger.info(\"Doing SVD...\")\n","    try:\n","        U,S,Vh = np.linalg.svd(Rstim, full_matrices=False)\n","    except np.linalg.LinAlgError as e:\n","        logger.info(\"NORMAL SVD FAILED, trying more robust dgesvd..\")\n","        from text.regression.svd_dgesvd import svd_dgesvd\n","        U,S,Vh = svd_dgesvd(Rstim, full_matrices=False)\n","\n","    ## Truncate tiny singular values for speed\n","    origsize = S.shape[0]\n","    ngoodS = np.sum(S>singcutoff)\n","    nbad = origsize-ngoodS\n","    U = U[:,:ngoodS]\n","    S = S[:ngoodS]\n","    Vh = Vh[:ngoodS]\n","    logger.info(\"Dropped %d tiny singular values.. (U is now %s)\"%(nbad, str(U.shape)))\n","\n","    ## Normalize alpha by the Frobenius norm\n","    #frob = np.sqrt((S**2).sum()) ## Frobenius!\n","    frob = S[0]\n","    #frob = S.sum()\n","    logger.info(\"Training stimulus has Frobenius norm: %0.03f\"%frob)\n","    if normalpha:\n","        nalphas = alphas * frob\n","    else:\n","        nalphas = alphas\n","\n","    ## Precompute some products for speed\n","    UR = np.dot(U.T, Rresp) ## Precompute this matrix product for speed\n","    PVh = np.dot(Pstim, Vh.T) ## Precompute this matrix product for speed\n","    \n","    #Prespnorms = np.apply_along_axis(np.linalg.norm, 0, Presp) ## Precompute test response norms\n","    zPresp = zs(Presp)\n","    Prespvar = Presp.var(0)\n","    Rcorrs = [] ## Holds training correlations for each alpha\n","    for na, a in zip(nalphas, alphas):\n","        #D = np.diag(S/(S**2+a**2)) ## Reweight singular vectors by the ridge parameter \n","        D = S/(S**2+na**2) ## Reweight singular vectors by the (normalized?) ridge parameter\n","        \n","        pred = np.dot(mult_diag(D, PVh, left=False), UR) ## Best (1.75 seconds to prediction in test)\n","        # pred = np.dot(mult_diag(D, np.dot(Pstim, Vh.T), left=False), UR) ## Better (2.0 seconds to prediction in test)\n","        \n","        # pvhd = reduce(np.dot, [Pstim, Vh.T, D]) ## Pretty good (2.4 seconds to prediction in test)\n","        # pred = np.dot(pvhd, UR)\n","        \n","        # wt = reduce(np.dot, [Vh.T, D, UR]).astype(dtype) ## Bad (14.2 seconds to prediction in test)\n","        # wt = reduce(np.dot, [Vh.T, D, U.T, Rresp]).astype(dtype) ## Worst\n","        # pred = np.dot(Pstim, wt) ## Predict test responses\n","\n","        if use_corr:\n","            #prednorms = np.apply_along_axis(np.linalg.norm, 0, pred) ## Compute predicted test response norms\n","            #Rcorr = np.array([np.corrcoef(Presp[:,ii], pred[:,ii].ravel())[0,1] for ii in range(Presp.shape[1])]) ## Slowly compute correlations\n","            #Rcorr = np.array(np.sum(np.multiply(Presp, pred), 0)).squeeze()/(prednorms*Prespnorms) ## Efficiently compute correlations\n","            Rcorr = (zPresp*zs(pred)).mean(0)\n","        else:\n","            ## Compute variance explained\n","            resvar = (Presp-pred).var(0)\n","            Rcorr = np.clip(1-(resvar/Prespvar), 0, 1)\n","            \n","        Rcorr[np.isnan(Rcorr)] = 0\n","        Rcorrs.append(Rcorr)\n","        \n","        log_template = \"Training: alpha=%0.3f, mean corr=%0.5f, max corr=%0.5f, over-under(%0.2f)=%d\"\n","        log_msg = log_template % (a,\n","                                  np.mean(Rcorr),\n","                                  np.max(Rcorr),\n","                                  corrmin,\n","                                  (Rcorr>corrmin).sum()-(-Rcorr>corrmin).sum())\n","        if logger is not None:\n","            logger.info(log_msg)\n","        else:\n","            print (log_msg)\n","    \n","    return Rcorrs\n","\n","\n","def bootstrap_ridge(Rstim, Rresp, Pstim, Presp, alphas, nboots, chunklen, nchunks, dtype=np.single,\n","                    corrmin=0.2, joined=None, singcutoff=1e-10, normalpha=False, single_alpha=False,\n","                    use_corr=True, logger=logging.getLogger(\"ridge_corr\")):\n","    \"\"\"Uses ridge regression with a bootstrapped held-out set to get optimal alpha values for each response.\n","    [nchunks] random chunks of length [chunklen] will be taken from [Rstim] and [Rresp] for each regression\n","    run.  [nboots] total regression runs will be performed.  The best alpha value for each response will be\n","    averaged across the bootstraps to estimate the best alpha for that response.\n","    \n","    If [joined] is given, it should be a list of lists where the STRFs for all the voxels in each sublist \n","    will be given the same regularization parameter (the one that is the best on average).\n","    \n","    Parameters\n","    ----------\n","    Rstim : array_like, shape (TR, N)\n","        Training stimuli with TR time points and N features. Each feature should be Z-scored across time.\n","    Rresp : array_like, shape (TR, M)\n","        Training responses with TR time points and M different responses (voxels, neurons, what-have-you).\n","        Each response should be Z-scored across time.\n","    Pstim : array_like, shape (TP, N)\n","        Test stimuli with TP time points and N features. Each feature should be Z-scored across time.\n","    Presp : array_like, shape (TP, M)\n","        Test responses with TP time points and M different responses. Each response should be Z-scored across\n","        time.\n","    alphas : list or array_like, shape (A,)\n","        Ridge parameters that will be tested. Should probably be log-spaced. np.logspace(0, 3, 20) works well.\n","    nboots : int\n","        The number of bootstrap samples to run. 15 to 30 works well.\n","    chunklen : int\n","        On each sample, the training data is broken into chunks of this length. This should be a few times \n","        longer than your delay/STRF. e.g. for a STRF with 3 delays, I use chunks of length 10.\n","    nchunks : int\n","        The number of training chunks held out to test ridge parameters for each bootstrap sample. The product\n","        of nchunks and chunklen is the total number of training samples held out for each sample, and this \n","        product should be about 20 percent of the total length of the training data.\n","    dtype : np.dtype\n","        All data will be cast as this dtype for computation. np.single is used by default for memory efficiency,\n","        as using np.double will thrash most machines on a big problem. If you want to do regression on \n","        complex variables, this should be changed to np.complex128.\n","    corrmin : float in [0..1]\n","        Purely for display purposes. After each alpha is tested for each bootstrap sample, the number of \n","        responses with correlation greater than this value will be printed. For long-running regressions this\n","        can give a rough sense of how well the model works before it's done.\n","    joined : None or list of array_like indices\n","        If you want the STRFs for two (or more) responses to be directly comparable, you need to ensure that\n","        the regularization parameter that they use is the same. To do that, supply a list of the response sets\n","        that should use the same ridge parameter here. For example, if you have four responses, joined could\n","        be [np.array([0,1]), np.array([2,3])], in which case responses 0 and 1 will use the same ridge parameter\n","        (which will be parameter that is best on average for those two), and likewise for responses 2 and 3.\n","    singcutoff : float\n","        The first step in ridge regression is computing the singular value decomposition (SVD) of the\n","        stimulus Rstim. If Rstim is not full rank, some singular values will be approximately equal\n","        to zero and the corresponding singular vectors will be noise. These singular values/vectors\n","        should be removed both for speed (the fewer multiplications the better!) and accuracy. Any\n","        singular values less than singcutoff will be removed.\n","    normalpha : boolean\n","        Whether ridge parameters (alphas) should be normalized by the Frobenius norm of Rstim. Good for rigorously\n","        comparing models with different numbers of parameters.\n","    single_alpha : boolean\n","        Whether to use a single alpha for all responses. Good for identification/decoding.\n","    use_corr : boolean\n","        If True, this function will use correlation as its metric of model fit. If False, this function\n","        will instead use variance explained (R-squared) as its metric of model fit. For ridge regression\n","        this can make a big difference -- highly regularized solutions will have very small norms and\n","        will thus explain very little variance while still leading to high correlations, as correlation\n","        is scale-free while R**2 is not.\n","    \n","    Returns\n","    -------\n","    wt : array_like, shape (N, M)\n","        Regression weights for N features and M responses.\n","    corrs : array_like, shape (M,)\n","        Validation set correlations. Predicted responses for the validation set are obtained using the regression\n","        weights: pred = np.dot(Pstim, wt), and then the correlation between each predicted response and each \n","        column in Presp is found.\n","    alphas : array_like, shape (M,)\n","        The regularization coefficient (alpha) selected for each voxel using bootstrap cross-validation.\n","    bootstrap_corrs : array_like, shape (A, M, B)\n","        Correlation between predicted and actual responses on randomly held out portions of the training set,\n","        for each of A alphas, M voxels, and B bootstrap samples.\n","    valinds : array_like, shape (TH, B)\n","        The indices of the training data that were used as \"validation\" for each bootstrap sample.\n","    \"\"\"\n","    nresp, nvox = Rresp.shape\n","    bestalphas = np.zeros((nboots, nvox))  ## Will hold the best alphas for each voxel\n","    valinds = [] ## Will hold the indices into the validation data for each bootstrap\n","    \n","    Rcmats = []\n","    for bi in counter(range(nboots), countevery=1, total=nboots):\n","        logger.info(\"Selecting held-out test set..\")\n","        allinds = range(nresp)\n","        indchunks = list(zip(*[iter(allinds)]*chunklen))\n","        random.shuffle(indchunks)\n","        heldinds = list(itools.chain(*indchunks[:nchunks]))\n","        notheldinds = list(set(allinds)-set(heldinds))\n","        valinds.append(heldinds)\n","        \n","        RRstim = Rstim[notheldinds,:]\n","        PRstim = Rstim[heldinds,:]\n","        RRresp = Rresp[notheldinds,:]\n","        PRresp = Rresp[heldinds,:]\n","        \n","        ## Run ridge regression using this test set\n","        Rcmat = ridge_corr(RRstim, PRstim, RRresp, PRresp, alphas,\n","                           dtype=dtype, corrmin=corrmin, singcutoff=singcutoff,\n","                           normalpha=normalpha, use_corr=use_corr)\n","        \n","        Rcmats.append(Rcmat)\n","    \n","    ## Find weights for each voxel\n","    try:\n","        U,S,Vh = np.linalg.svd(Rstim, full_matrices=False)\n","    except np.linalg.LinAlgError as e:\n","        logger.info(\"NORMAL SVD FAILED, trying more robust dgesvd..\")\n","        from text.regression.svd_dgesvd import svd_dgesvd\n","        U,S,Vh = svd_dgesvd(Rstim, full_matrices=False)\n","\n","    ## Normalize alpha by the Frobenius norm\n","    #frob = np.sqrt((S**2).sum()) ## Frobenius!\n","    frob = S[0]\n","    #frob = S.sum()\n","    logger.info(\"Total training stimulus has Frobenius norm: %0.03f\"%frob)\n","    if normalpha:\n","        nalphas = alphas * frob\n","    else:\n","        nalphas = alphas\n","\n","    allRcorrs = np.dstack(Rcmats)\n","    if not single_alpha:\n","        logger.info(\"Finding best alpha for each response..\")\n","        if joined is None:\n","            ## Find best alpha for each voxel\n","            meanbootcorrs = allRcorrs.mean(2)\n","            bestalphainds = np.argmax(meanbootcorrs, 0)\n","            valphas = nalphas[bestalphainds]\n","        else:\n","            ## Find best alpha for each group of voxels\n","            valphas = np.zeros((nvox,))\n","            for jl in joined:\n","                jcorrs = allRcorrs[:,jl,:].mean(1).mean(1) ## Mean across voxels in the set, then mean across bootstraps\n","                bestalpha = np.argmax(jcorrs)\n","                valphas[jl] = nalphas[bestalpha]\n","    else:\n","        logger.info(\"Finding single best alpha..\")\n","        meanbootcorr = allRcorrs.mean(2).mean(1)\n","        bestalphaind = np.argmax(meanbootcorr)\n","        bestalpha = alphas[bestalphaind]\n","        valphas = np.array([bestalpha]*nvox)\n","        logger.info(\"Best alpha = %0.3f\"%bestalpha)\n","\n","    logger.info(\"Computing weights for each response using entire training set..\")\n","    UR = np.dot(U.T, np.nan_to_num(Rresp))\n","    pred = np.zeros(Presp.shape)\n","    wt = np.zeros((Rstim.shape[1], Rresp.shape[1]))\n","    for ai,alpha in enumerate(nalphas):\n","        selvox = np.nonzero(valphas==alpha)[0]\n","        awt = reduce(np.dot, [Vh.T, np.diag(S/(S**2+alpha**2)), UR[:,selvox]])\n","        pred[:,selvox] = np.dot(Pstim, awt)\n","        wt[:,selvox] = awt\n","\n","    ## Find test correlations\n","    nnpred = np.nan_to_num(pred)\n","    corrs = np.nan_to_num(np.array([np.corrcoef(Presp[:,ii], nnpred[:,ii].ravel())[0,1] for ii in range(Presp.shape[1])]))\n","\n","    return wt, corrs, valphas, allRcorrs, valinds\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"q1ev5EL0MsJY"},"source":["!cd #to working dir\n","!pip install patool\n","import patoolib\n","patoolib.extract_archive(\"layer_{}.rar\", outdir=\"path here\") \n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rNQ_T4CMiQQN","cellView":"form"},"source":["#@title Utility functions for regression\n","def vectorized_correlation(x,y):\n","    dim = 0\n","\n","    centered_x = x - x.mean(axis=dim, keepdims=True)\n","    centered_y = y - y.mean(axis=dim, keepdims=True)\n","\n","    covariance = (centered_x * centered_y).sum(axis=dim, keepdims=True)\n","\n","    bessel_corrected_covariance = covariance / (x.shape[dim] - 1)\n","\n","    x_std = x.std(axis=dim, keepdims=True)+1e-8\n","    y_std = y.std(axis=dim, keepdims=True)+1e-8\n","\n","    corr = bessel_corrected_covariance / (x_std * y_std)\n","\n","    return corr.ravel()\n","\n","class OLS_pytorch(object):\n","    def __init__(self,use_gpu=False):\n","        self.coefficients = []\n","        self.use_gpu = use_gpu\n","        self.X = None\n","        self.y = None\n","\n","    def fit(self,X,y):\n","        if len(X.shape) == 1:\n","            X = self._reshape_x(X)\n","        if len(y.shape) == 1:\n","            y = self._reshape_x(y)\n","\n","        X =  self._concatenate_ones(X)\n","\n","        X = torch.from_numpy(X).float()\n","        y = torch.from_numpy(y).float()\n","        if self.use_gpu:\n","            X = X.cuda()\n","            y = y.cuda()\n","        XtX = torch.matmul(X.t(),X)\n","        Xty = torch.matmul(X.t(),y.unsqueeze(2))\n","        XtX = XtX.unsqueeze(0)\n","        XtX = torch.repeat_interleave(XtX, y.shape[0], dim=0)\n","        betas_cholesky, _ = torch.solve(Xty, XtX)\n","\n","        self.coefficients = betas_cholesky\n","\n","    def predict(self, entry):\n","        if len(entry.shape) == 1:\n","            entry = self._reshape_x(entry)\n","        entry =  self._concatenate_ones(entry)\n","        entry = torch.from_numpy(entry).float()\n","        if self.use_gpu:\n","            entry = entry.cuda()\n","        prediction = torch.matmul(entry,self.coefficients)\n","        prediction = prediction.cpu().numpy()\n","        prediction = np.squeeze(prediction).T\n","        return prediction\n","\n","    def _reshape_x(self,X):\n","        return X.reshape(-1,1)\n","\n","    def _concatenate_ones(self,X):\n","        ones = np.ones(shape=X.shape[0]).reshape(-1,1)\n","        return np.concatenate((ones,X),1)\n","\n","def predict_fmri_fast(train_activations, test_activations, train_fmri,use_gpu=False):\n","    \"\"\"This function fits a linear regressor using train_activations and train_fmri,\n","    then returns the predicted fmri_pred_test using the fitted weights and\n","    test_activations.\n","    Parameters\n","    ----------\n","    train_activations : np.array\n","        matrix of dimensions #train_vids x #pca_components\n","        containing activations of train videos.\n","    test_activations : np.array\n","        matrix of dimensions #test_vids x #pca_components\n","        containing activations of test videos\n","    train_fmri : np.array\n","        matrix of dimensions #train_vids x  #voxels\n","        containing fMRI responses to train videos\n","    use_gpu : bool\n","        Description of parameter `use_gpu`.\n","    Returns\n","    -------\n","    fmri_pred_test: np.array\n","        matrix of dimensions #test_vids x  #voxels\n","        containing predicted fMRI responses to test videos .\n","    \"\"\"\n","\n","    reg = OLS_pytorch(use_gpu)\n","    reg.fit(train_activations,train_fmri.T)\n","    fmri_pred_test = reg.predict(test_activations)\n","    # betas = reg.coef_\n","    # np.savez_compressed(betas)\n","    return fmri_pred_test\n","\n","            "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9qkCflh9Ymy1"},"source":["def perform_encoding(activation_dir, fmri_dir,results_dir, sub, layer, ROI = 'WB', mode = 'val', visualize_results = True\n","                     , batch_size=1000):\n","  if torch.cuda.is_available():\n","      use_gpu = True\n","  else:\n","      use_gpu = False\n","\n","  ###### Load activations ##############\n","  activations_dir = '/content/drive/MyDrive/Colab Notebooks/DumboOctopi/AlgonautsDataExtraction/resnetBasline/activations_resnet18'\n","  pca_dir = os.path.join(activations_dir,'pca_100')\n","  train_activations,test_activations = get_activations(pca_dir, layer)\n","  ######################################\n","\n","  ##### Load fMRI data #################\n","  if ROI == \"WB\":\n","      track = \"full_track\"\n","  else:\n","      track = \"mini_track\"\n","  fmri_dir = os.path.join(fmri_dir, track)\n","  sub_fmri_dir = os.path.join(fmri_dir, sub)\n","  if track == \"full_track\":\n","      fmri_train_all,voxel_mask = get_fmri(sub_fmri_dir,ROI)\n","  else:\n","      fmri_train_all = get_fmri(sub_fmri_dir,ROI)\n","  num_voxels = fmri_train_all.shape[1]\n","  ######################################\n","\n","\n","  #### Creating data splits ###############\n","  if mode == 'val':\n","      # Here as an example we use first 900 videos as training and rest of the videos as validation\n","      test_activations = train_activations[900:,:]\n","      train_activations = train_activations[:900,:]\n","      fmri_train = fmri_train_all[:900,:]\n","      fmri_test = fmri_train_all[900:,:]\n","      pred_fmri = np.zeros_like(fmri_test)\n","      pred_fmri_save_path = os.path.join(results_dir, ROI + '_val.npy')\n","  else:\n","      fmri_train = fmri_train_all\n","      num_test_videos = 102\n","      pred_fmri = np.zeros((num_test_videos,num_voxels))\n","      pred_fmri_save_path = os.path.join(results_dir, ROI + '_test.npy')\n","  ######################################\n","\n","  ######## Performing regression ################\n","  iter = 0\n","  \n","  while iter < num_voxels-batch_size:\n","      pred_fmri[:,iter:iter+batch_size] = predict_fmri_fast(train_activations,test_activations,fmri_train[:,iter:iter+batch_size], use_gpu = use_gpu)\n","      iter = iter+batch_size\n","  pred_fmri[:,iter:] = predict_fmri_fast(train_activations,test_activations,fmri_train[:,iter:iter+batch_size], use_gpu = use_gpu)\n","  if mode == 'val':\n","    score = vectorized_correlation(fmri_test,pred_fmri)\n","    print(\"----------------------------------------------------------------------------\")\n","    print(\"Mean correlation for ROI : \",ROI, \"in \",sub,\" using \",layer, \" is :\", round(score.mean(), 6))\n","    ################################################\n","    \n","    nii_save_path =  os.path.join(results_dir, ROI + '_val.nii')\n","    ######## Result visualization ################\n","    if track == \"full_track\" and visualize_results:\n","        visual_mask_3D = np.zeros((78,93,71))\n","        visual_mask_3D[voxel_mask==1]= score\n","        brain_mask = '/content/example.nii'\n","        saveasnii(brain_mask,nii_save_path,visual_mask_3D)\n","        plotting.plot_glass_brain(nii_save_path,plot_abs=False,\n","                          title='Correlation for ' + sub+ ' and ' + layer,\n","                          display_mode='lyr',colorbar=True,vmin=-1,vmax=1)\n","\n","    ################################################\n","\n","  np.save(pred_fmri_save_path, pred_fmri)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8zpVK1zWiGAt"},"source":["# Predict fMRI responses to test videos for all subjects all ROIs\n","Run the cell below to save predicted fMRI responses to test videos for all subjects and ROIs."]},{"cell_type":"code","metadata":{"id":"0-_DNz-NiNVw"},"source":["subs = [\"sub01\",\"sub02\",\"sub03\",\"sub04\",\"sub05\",\"sub06\",\"sub07\",\"sub08\",\"sub09\",\"sub10\"]  \n","ROIs = [\"WB\", \"V1\", \"V2\",\"V3\", \"V4\", \"LOC\", \"EBA\", \"FFA\",\"STS\", \"PPA\"]\n","layers = [\"layer_1\",\"layer_2\",\"layer_3\",\"layer_4\",\"layer_5\",\"layer_6\",\"layer_7\",\"layer_8\"]\n","\n","fmri_dir = '/content/participants_data_v2021'\n","prediction_dir = '/content/drive/MyDrive/Colab Notebooks/DumboOctopi/AlgonautsDataExtraction/resnetBasline/prediction'\n","model = 'resnet18'\n","\n","for layer in layers:\n","  for sub in subs:\n","    for ROI in ROIs:\n","      if ROI == \"WB\":\n","          track = \"full_track\"\n","      else:\n","          track = \"mini_track\"\n","      results_dir = os.path.join(prediction_dir,model, layer,\\\n","                            track, sub)\n","      if not os.path.exists(results_dir):\n","        os.makedirs(results_dir)\n","      print (\"Starting ROI: \", ROI, \"sub: \",sub)\n","      perform_encoding(activations_dir, fmri_dir,\\\n","                      results_dir, sub, layer,\\\n","                      ROI=ROI,mode='test')\n","      print (\"Completed ROI: \", ROI, \"sub: \",sub)\n","      print(\"----------------------------------------------------------------------------\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qRLvvEwnWxzS"},"source":["\n","subIDs = [\"sub01\",\"sub02\",\"sub03\",\"sub04\",\"sub05\",\"sub06\",\"sub07\",\"sub08\",\"sub09\",\"sub10\"]  \n","ROIs = [\"WB\"]\n","# ROIs = [\"V1\", \"V2\",\"V3\", \"V4\", \"LOC\", \"EBA\", \"FFA\",\"STS\", \"PPA\"]\n","layers = [\"layer_1\",\"layer_2\",\"layer_3\",\"layer_4\",\"layer_5\",\"layer_6\",\"layer_7\",\"layer_8\"]\n","\n","df = np.zeros((len(subIDs),len(ROIs)))\n","for layer in layers:\n","  for i, subID in enumerate(subIDs): \n","    for j, ROI in enumerate(ROIs):\n","      outnp = np.load(f'/content/drive/MyDrive/Colab Notebooks/DumboOctopi/AlgonautsDataExtraction/resnetBasline/prediction/resnet18/{layer}/full_track/{subID}/{ROI}_test.npy')\n","      # outnp = np.load(f'/content/drive/MyDrive/Colab Notebooks/DumboOctopi/AlgonautsDataExtraction/resnetBasline/prediction/resnet18/{layer}/mini_track/{subID}/{ROI}_test.npy')\n","      # print(outnp.shape)\n","      mn = outnp.mean()\n","      df[i,j] = mn\n","\n","  print(df.shape)\n","\n","  # plt.imshow(df, cmap='hot')\n","\n","  pos = np.arange(0,9)\n","  plt.figure(figsize=(15,8))\n","  plt.boxplot(df, labels = ROIs)\n","  plt.title(f'{layer}')\n","  plt.savefig(f'/content/drive/MyDrive/Colab Notebooks/DumboOctopi/AlgonautsDataExtraction/resnetBasline/prediction/resnet18/BoxPlots/{layer}_full.png')\n","    # plt.savefig(f'/content/drive/MyDrive/Colab Notebooks/DumboOctopi/AlgonautsDataExtraction/resnetBasline/prediction/resnet18/BoxPlots/{layer}_mini.png')\n","  plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Gx4Cs4dcmdzc"},"source":["# lay8 = plt.imread('/content/prediction/resnet18/results/layer8_mini.png')\n","# plt.figure(figsize=(15,8))\n","# plt.imshow(lay8)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ris61CeobR67"},"source":["#Reference\n","\n","[The Algonauts Project 2021 Challenge: How the Human Brain Makes Sense of a World in Motion. \n","*Cichy et al. arxiv 2021*](https://arxiv.org/abs/2104.13714v1)\n","\n","\n"]}]}